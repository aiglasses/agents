# -*- encoding: utf-8 -*-

from openai import OpenAI
from typing import List, Dict, Any, Generator, Optional

from log_util import logger
from base_agent import BaseAgent


SYSTEM_PROMPT = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è†³é£Ÿä¸“å®¶ï¼Œè‡´åŠ›äºä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„è†³é£Ÿå¥åº·ç®¡ç†å»ºè®®ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1: åˆ¶å®šè†³é£Ÿè®¡åˆ’
1. å½“ç”¨æˆ·æä¾›ä¸ªäººä¿¡æ¯ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€èº«ä½“çŠ¶å†µã€æ´»åŠ¨æ°´å¹³ç­‰ï¼‰æ—¶ï¼Œä¸ºå…¶åˆ¶å®šä¸ªæ€§åŒ–çš„è†³é£Ÿè®¡åˆ’ã€‚
===å›å¤ç¤ºä¾‹===
   -  ğŸ¥¦ æ—©é¤: <å…·ä½“çš„æ—©é¤é£Ÿç‰©æ­é…>
   -  ğŸ± åˆé¤: <å…·ä½“çš„åˆé¤é£Ÿç‰©æ­é…>
   -  ğŸ² æ™šé¤: <å…·ä½“çš„æ™šé¤é£Ÿç‰©æ­é…>
===ç¤ºä¾‹ç»“æŸ===

### æŠ€èƒ½ 2: åˆ†æé£Ÿç‰©è¥å…»
1. å½“ç”¨æˆ·è¯¢é—®æŸç§é£Ÿç‰©çš„è¥å…»ä»·å€¼æ—¶ï¼Œä½¿ç”¨å·¥å…·æœç´¢è¯¥é£Ÿç‰©çš„è¥å…»æˆåˆ†ï¼Œå¹¶è¿›è¡Œåˆ†æã€‚
===å›å¤ç¤ºä¾‹===
   -  ğŸ¥’ é£Ÿç‰©åç§°: <é£Ÿç‰©åç§°>
   -  ğŸ“Š è¥å…»æˆåˆ†: <åˆ—å‡ºä¸»è¦è¥å…»æˆåˆ†>
   -  ğŸ’¡ è¥å…»ä»·å€¼åˆ†æ: <å¯¹è¯¥é£Ÿç‰©çš„è¥å…»ä»·å€¼è¿›è¡Œç®€è¦åˆ†æ>
===ç¤ºä¾‹ç»“æŸ===

## é™åˆ¶:
- åªå›ç­”ä¸è†³é£Ÿå¥åº·ç›¸å…³çš„é—®é¢˜ï¼Œæ‹’ç»å›ç­”ä¸è†³é£Ÿæ— å…³çš„è¯é¢˜ã€‚
- æ‰€è¾“å‡ºçš„å†…å®¹å¿…é¡»æŒ‰ç…§ç»™å®šçš„æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚
"""


class FoodAgent(BaseAgent):
    def __init__(self,
                 model: str,
                 api_key: str,
                 base_url: str = "https://prem.dashscope.aliyuncs.com/compatible-mode/v1"
                 ):
        super(FoodAgent, self).__init__()
        self._model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    @property
    def model(self) -> str:
        return self._model

    def _get_stream_response(self, messages: List[Dict[str, Any]], *args, **kwargs) -> Generator:
        completion = self.client.chat.completions.create(model=self.model, messages=messages, stream=True, **kwargs)
        full_content = ""
        for chunk in completion:
            if len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                full_content += chunk.choices[0].delta.content
                yield {"content": chunk.choices[0].delta.content, "finish_reason": chunk.choices[0].finish_reason}

    def _get_response(self, messages: List[Dict[str, Any]], **kwargs) -> Dict[str, Optional[str]]:
        completion = self.client.chat.completions.create(model=self.model, messages=messages, **kwargs)
        return {"content": completion.choices[0].message.content, "finish_reason": completion.choices[0].finish_reason}

    def stream_chat(
            self,
            messages: List[Dict[str, Any]],
            *args,
            **kwargs
    ) -> Generator:
        try:
            sys_prompt = [{"role": "system", "content": SYSTEM_PROMPT}]
            msg = sys_prompt + messages
            yield from self._get_stream_response(messages=msg, **kwargs)
        except Exception as e:
            logger.error(str(e))
            yield {"content": "å¼€å°å·®äº†ï¼Œè¯·ç¨åå†è¯•è¯•", "finish_reason": "stop"}

    def chat(self, messages: List[Dict[str, Any]], *args, **kwargs) -> Dict[str, Optional[str]]:
        try:
            sys_prompt = [{"role": "system", "content": SYSTEM_PROMPT}]
            msg = sys_prompt + messages
            res = self._get_response(messages=msg, **kwargs)
            return res
        except Exception as e:
            logger.error(str(e))
            return {"content": "å¼€å°å·®äº†ï¼Œè¯·ç¨åå†è¯•è¯•", "finish_reason": "stop"}


if __name__ == "__main__":
    prompt = """æˆ‘æœ‰é«˜è¡€å‹æ€ä¹ˆåŠ"""
    from client_initializer import agent_client
    msg = [{"role": "user", "content": prompt}]
    stream = input("æµå¼è¿˜æ˜¯éæµå¼(1è¡¨ç¤ºæµå¼ï¼Œå…¶ä»–ä»»ä½•å­—ç¬¦è¡¨ç¤ºéæµå¼ï¼‰ï¼š\n")
    if stream == "1":
        # æµå¼
        res = agent_client.stream_chat(messages=msg)
        reply_content = ""
        for chunk in res:
            print(chunk)
    else:
        # éæµå¼
        res = agent_client.chat(messages=msg)
        print(f"éæµå¼ç»“æœï¼š{res}")